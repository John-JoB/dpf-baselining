{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Pydpf Basics Tutorial\n",
    "This tutorial gives a basic overview of the differentiable particle filtering package, pydpf. The package is intended to implement the majority of differentiable filters currently available in the literature and provide a convenient API for training them. It is part of a larger project to develop a benchmarking suite for DPFs. This tutorial assumes good knowledge of pytorch.\n",
    "\n",
    "## Design principles\n",
    "1. Extensibility\n",
    "    It should be as easy as possible for a user to define and train a filter to their needs without the package getting in the way. It should generally be possible to extend the functionality we provide by passing values to package functions, rather than subclassing package classes.\n",
    "2. Pytorch semantics\n",
    "    This module integrates with pytorch. Typical torch semantics are preserved as much as possible.\n",
    "3. Learning functions not models.\n",
    "    Conceptually it is best to think of modules written for pydpf as learning an algorithm to achieve the given inference goal, not a specific state space model. This gives the user flexibility in structuring their model as best suited to their problem rather than having to conform to the package's syntax. Most particle filtering algorithms are implementable as 'models' that can be passed to our base SMC algorithms, rather than needing to design them from scratch. Conversely, this puts the responsibility of designing well-structured and correct filtering algorithms on the user.\n",
    "\n",
    "## Overarching design patterns\n",
    "\n",
    "### The pydpf.Module class\n",
    "pydpf has a custom Module class that extends torch.nn.Module. pydpf.Module subclasses can optionally contain an update() method that should be used to calculate derived quantities from, or constrain parameters after they have changed. We recommend that all custom modules defined for use with pydpf subclass pydpf.Module. update() is called recursively on submodules, for this reason it is safe for pydf.Module classes to have torch.nn.Module submodules but not vice versa. So, there is no issue in using built in torch modules. Consequently, it is not safe to directly set attributes of a Module, one should provide a safe set_attribute method for Module attributes. The exception to this rule is when using torch's (somewhat arcane) optimizers which update the parameters inplace. One should then make sure to call .update() on all root Modules following an Optimizer step.\n",
    "\n",
    "### Dataformat\n",
    "@Ben this is subject to change if required. The data-format currently required by the pydpf data loading faculties is a folder of .csv files where each file contains a trajectory. Each file should have two columns, labelled state and observations containing vectors in the format '[state_1, state_2, ..., state_n]' (quotes included). Other columns are permitted but will be ignored.  \n",
    "\n",
    "### Order of dimensions\n",
    "The order of dimensions shall be: time, batch, particle, state/observation-dimension. Frequently one or more of these dimensions shall not be present but they should not change relative order."
   ],
   "id": "1374b35513731711"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T13:50:31.472569Z",
     "start_time": "2024-10-27T13:50:25.816542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#imports\n",
    "import pydpf\n",
    "import torch\n",
    "from typing import Tuple\n",
    "from pydpf import Module, constrained_parameter\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "#set device, defaults to cuda if it is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#set data path\n",
    "data_path = \"./data\""
   ],
   "id": "7a27496c771ae113",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Defining the true Model\n",
    "This tutorial takes the user through a simple two-dimensional Linear Gaussian example.\n",
    "At time = 0 the state is drawn from a Gaussian, at subsequent time-steps it is drawn from a Gaussian Markov kernel with linear dependence on the previous state. The observations are drawn from a different linear Gaussian forward kernel. The linear transforms and covariance matrices are constant for all time steps. "
   ],
   "id": "801d3cc4d4ba42b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T13:50:31.653653Z",
     "start_time": "2024-10-27T13:50:31.478513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Define the true parameters\n",
    "true_prior_mean = torch.zeros(size = (2,), dtype = torch.float32, device = device)\n",
    "true_prior_cholesky_cov = torch.eye(2, dtype = torch.float32, device = device)\n",
    "true_dynamic_scaling = torch.eye(n=2, dtype = torch.float32, device = device) * 0.9\n",
    "true_dynamic_offset = torch.tensor([0.2, 0.1], dtype = torch.float32, device = device)\n",
    "true_dynamic_cholesky_cov = torch.tensor([[0.1, 0], [0.03, 0.1]], dtype = torch.float32, device = device)\n",
    "true_observation_scaling = torch.eye(n = 2, dtype = torch.float32, device = device)\n",
    "true_observation_offset = torch.tensor([0, 0], dtype = torch.float32, device = device)\n",
    "true_observation_cholesky_cov = torch.eye(2, dtype = torch.float32, device = device)*0.05"
   ],
   "id": "d820bb644105c242",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## The distributions module\n",
    "We include a distributions module to implement common distributions (and conditional distributions) with a convenient API. Like the torch.distributions package it is not intended for the user to subclass the Distribution object, if they wish to implement their own it will always be easier to manually define sampling and density evaluation methods. Distribution objects implement a sample() and log_density() method, the specific arguments of which depend on the specific distribution. Distributions are pydpf.Module subclasses so may be assigned parameters. Distributions give three built in options for the gradient estimator to use for sampling, 'reparameterisation' reparameterisation trick, 'score' score-based/REINFORCE, and 'none' detach gradients. Although, attaching a score-based gradient estimator to the sample requires doing so in linear-space, whereas attaching it to the importance weights can be done in log-space so may be more stable in practice."
   ],
   "id": "205f7b890fe98f3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T13:50:32.266025Z",
     "start_time": "2024-10-27T13:50:32.259728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "true_prior = pydpf.distributions.MultivariateGaussian(gradient_estimator = 'none', generator = None, mean = true_prior_mean, cholesky_covariance = true_prior_cholesky_cov)\n",
    "\n",
    "true_dynamic = pydpf.distributions.LinearGaussian(gradient_estimator='none', generator= None, weight =  true_dynamic_scaling, bias = true_dynamic_offset, cholesky_covariance = true_dynamic_cholesky_cov)\n",
    "\n",
    "true_observation = pydpf.distributions.LinearGaussian(gradient_estimator='none', generator= None, weight =  true_observation_scaling, bias = true_observation_offset, cholesky_covariance = true_observation_cholesky_cov)\n",
    "\n",
    "def dynamic_sampler(prev_state, t):\n",
    "    return true_dynamic.sample(prev_state, None)\n",
    "\n",
    "def observation_sampler(state, t):\n",
    "    return true_observation.sample(state, None)"
   ],
   "id": "8daa135a74e0fe05",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Saving simulated data\n",
    "We provide a method to generate a set of trajectories and save them to folder as .csv files as required by our format. The simulate_to_folder method takes methods to sample from the prior and the dynamic and observation kernels.\n",
    "\n",
    "In this case the sampling functions are the .sample() methods of the distributions defined above.\n",
    "\n",
    "Note: sometimes running joblib processes in a jupyter notebook crashes, just rerun the cell should this happen."
   ],
   "id": "2cee0e9c68d8a48d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T13:50:35.010859Z",
     "start_time": "2024-10-27T13:50:32.289202Z"
    }
   },
   "cell_type": "code",
   "source": "pydpf.simulate_to_folder(data_path, true_prior.sample, dynamic_sampler, observation_sampler, time_extent=50, n_trajectories=1000, batch_size=30, device=device, processes=-1)",
   "id": "3b8857bbfe177867",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - folder already exists at ./data, continuing could overwrite its data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m pydpf\u001B[38;5;241m.\u001B[39msimulate_to_folder(data_path, true_prior\u001B[38;5;241m.\u001B[39msample, dynamic_sampler, observation_sampler, time_extent\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m, n_trajectories\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m30\u001B[39m, device\u001B[38;5;241m=\u001B[39mdevice, processes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\dpf-baselining\\pydpf\\datautils.py:80\u001B[0m, in \u001B[0;36msimulate_to_folder\u001B[1;34m(dir_path, prior, Markov_kernel, observation_model, time_extent, n_trajectories, batch_size, device, processes)\u001B[0m\n\u001B[0;32m     78\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(dir_path):\n\u001B[0;32m     79\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWarning - folder already exists at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdir_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, continuing could overwrite its data\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 80\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mContinue? (y/n) \u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     81\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m response \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mY\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m response \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m     82\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHalting\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001B[0m, in \u001B[0;36mKernel.raw_input\u001B[1;34m(self, prompt)\u001B[0m\n\u001B[0;32m   1280\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw_input was called, but this frontend does not support input requests.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1281\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m StdinNotImplementedError(msg)\n\u001B[1;32m-> 1282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_request(\n\u001B[0;32m   1283\u001B[0m     \u001B[38;5;28mstr\u001B[39m(prompt),\n\u001B[0;32m   1284\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_parent_ident[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshell\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m   1285\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_parent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshell\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m   1286\u001B[0m     password\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m   1287\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001B[0m, in \u001B[0;36mKernel._input_request\u001B[1;34m(self, prompt, ident, parent, password)\u001B[0m\n\u001B[0;32m   1322\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[0;32m   1323\u001B[0m     \u001B[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001B[39;00m\n\u001B[0;32m   1324\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInterrupted by user\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1325\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1326\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m   1327\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid Message:\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: Interrupted by user"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Initialising model parameters\n",
    "In this example we will only learn the dynamic and observation models, not the prior. We randomly initialise the parameters as torch.nn.Parameters and define new distributions to hold them."
   ],
   "id": "50b0b92a2e0de33c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "initialisation_generator = torch.Generator(device = device).manual_seed(0)\n",
    "learned_dynamic_scaling = torch.nn.Parameter(torch.rand(size = (2,2), dtype = torch.float32, device = device, generator=initialisation_generator) * 2 - 1, requires_grad = True)\n",
    "learned_dynamic_offset = torch.nn.Parameter(torch.rand(size = (2,), dtype = torch.float32, device = device, generator=initialisation_generator) * 2 - 1, requires_grad = True, )\n",
    "learned_dynamic_cholesky_cov = torch.nn.Parameter(torch.rand(size = (2,2), dtype = torch.float32, device = device, generator=initialisation_generator), requires_grad = True)\n",
    "learned_observation_scaling = torch.nn.Parameter(torch.rand(size = (2,2), dtype = torch.float32, device = device, generator=initialisation_generator) * 2 - 1, requires_grad = True)\n",
    "learned_observation_offset = torch.nn.Parameter(torch.rand(size = (2,), dtype = torch.float32, device = device, generator=initialisation_generator) * 2 - 1, requires_grad = True)\n",
    "learned_observation_cholesky_cov = torch.nn.Parameter(torch.rand(size = (2,2), dtype = torch.float32, device = device, generator=initialisation_generator), requires_grad = True)\n",
    "\n",
    "prior_rng = torch.Generator(device = device)\n",
    "prior_rng.manual_seed(0)\n",
    "dynamic_rng = torch.Generator(device = device)\n",
    "dynamic_rng.manual_seed(1)\n",
    "observation_rng = torch.Generator(device = device)\n",
    "observation_rng.manual_seed(2)\n",
    "\n",
    "\n",
    "filtering_prior = true_prior\n",
    "\n",
    "\n",
    "filtering_dynamic = pydpf.distributions.LinearGaussian(gradient_estimator = 'reparameterisation', generator = dynamic_rng, weight = learned_dynamic_scaling, bias = learned_dynamic_offset, cholesky_covariance = learned_dynamic_cholesky_cov)\n",
    "\n",
    "filtering_observation = pydpf.distributions.LinearGaussian(gradient_estimator = 'reparameterisation', generator = observation_rng, weight = learned_observation_scaling, bias = learned_observation_offset, cholesky_covariance = learned_observation_cholesky_cov)"
   ],
   "id": "fa371e34e586adf0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Defining the DPF\n",
    "ParticleFilter is our implementation of a (not necessarily differentiable) particle filter. The particle filter is defined in terms of three functions. A function to importance sample posterior at time zero; takes the number of particles and the observation at time zero. A function to importance sample from the posterior at subsequent time-steps; takes the previous states, the previous weights, the observations at the current time and the current time. And a resampling algorithm; takes the particles and weights and returns the resampled particles, weights and another tensor (used to pass other information for outputting, most often the resampled indices).\n",
    "\n",
    "In our example we demonstrate the usage of the .update() function by using it to constrain the dynamic matrix's spectral index. For this example we employ the non-differentiable multinomial resampling."
   ],
   "id": "d257f27a75eefe1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class PriorSampler(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prior_model = filtering_prior\n",
    "        self.observation_model = filtering_observation\n",
    "\n",
    "    def forward(self, n_particles: int, data: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        new_state = self.prior_model.sample((data.size(0), n_particles))\n",
    "        new_weights = self.observation_model.log_density(data.unsqueeze(1), new_state)\n",
    "        return new_state, new_weights\n",
    "    \n",
    "    \n",
    "\n",
    "class FilteringSampler(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dynamic_model = filtering_dynamic\n",
    "        self.observation_model = filtering_observation\n",
    "        \n",
    "    def forward(self, state: torch.Tensor, weights: torch.Tensor, data: torch.Tensor, time: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        new_state = self.dynamic_model.sample(state)\n",
    "        new_weights = self.observation_model.log_density(data.unsqueeze(1), new_state) + weights\n",
    "        return new_state, new_weights\n",
    "        \n",
    "    @constrained_parameter\n",
    "    def constrained_params(self):\n",
    "        dynamic_eigvals = torch.linalg.eigvals(self.dynamic_model.weight)\n",
    "        dynamic_spectral_radius = torch.max(torch.abs(dynamic_eigvals))\n",
    "        if dynamic_spectral_radius > 1:\n",
    "            return self.dynamic_model.weight, self.dynamic_model.weight/dynamic_spectral_radius\n",
    "        return self.dynamic_model.weight, self.dynamic_model.weight\n",
    "\n",
    "resampling_generator = torch.Generator(device=device).manual_seed(6)\n",
    "\n",
    "dpf = pydpf.ParticleFilter(PriorSampler(), pydpf.multinomial(resampling_generator), FilteringSampler())"
   ],
   "id": "2895aacda6d65240",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Loading data\n",
    "Data loading closely follows pytorch syntax, we have defined a custom dataset to load data from a folder of formatted .csv files. The dataset can then be treated as a regular pytorch dataset. With the exception that one should pass its .collate() routine to the DataLoader collate_fn argument."
   ],
   "id": "26c56b69ffb46a8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset_generator = torch.Generator(device='cpu').manual_seed(3)\n",
    "train_loader_generator = torch.Generator(device='cpu').manual_seed(4)\n",
    "validation_loader_generator = torch.Generator(device='cpu').manual_seed(5)\n",
    "dataset = pydpf.StateSpaceDataset(data_path, device=device)\n",
    "train_set, validation_set = torch.utils.data.random_split(dataset, [500, 500], generator=dataset_generator)\n",
    "train_dataloader = DataLoader(train_set, batch_size=32, shuffle=True, generator=train_loader_generator, collate_fn=dataset.collate)\n",
    "validation_dataloader = DataLoader(validation_set, batch_size=128, shuffle=True, generator=validation_loader_generator, collate_fn=dataset.collate)"
   ],
   "id": "6341c85c2afc7669",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Hyper-parameters\n",
    "epochs = 20\n",
    "lr = 0.001\n",
    "n_particles_train = 100\n",
    "n_particles_validation = 1000"
   ],
   "id": "6cd2f6841b209b9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training loop\n",
    "It is often crucial to have control over the training loop to debug and tune models so, like base pytorch, we leave their implementation to the user. The simple algorithm in this example is not very good, it is only for the point of demonstration, much better losses for this example are possible."
   ],
   "id": "783c3807f61fa898"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "opt = torch.optim.SGD(dpf.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = []\n",
    "    for state, observation in train_dataloader:\n",
    "        dpf.update()\n",
    "        opt.zero_grad()\n",
    "        filtering_means = dpf(observation, n_particles_train, observation.size(0)-1, pydpf.filtering_mean())\n",
    "        loss = pydpf.MSE_loss(filtering_means, state)\n",
    "        loss.backward()\n",
    "        train_loss.append(loss.item())\n",
    "        opt.step()\n",
    "    train_loss = np.mean(np.array(train_loss))\n",
    "    dpf.update()\n",
    "    with torch.inference_mode():\n",
    "        for state, observation in validation_dataloader:\n",
    "            filtering_means = dpf(observation, n_particles_validation, observation.size(0) - 1, pydpf.filtering_mean())\n",
    "            loss = pydpf.MSE_loss(filtering_means, state)\n",
    "            validation_loss = loss.item()\n",
    "    print('                                                                                                    ', end='\\r')\n",
    "    print(f'epoch {epoch+1}/{epochs}, train loss: {train_loss}, validation loss: {validation_loss}', end='\\r')\n",
    "    \n",
    "print('')"
   ],
   "id": "fb5954232952e84d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-30T13:08:37.505376Z",
     "start_time": "2025-01-30T13:08:35.999974Z"
    }
   },
   "source": [
    "import pydpf\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import Tuple, Union\n",
    "from pydpf.datautils import StateSpaceDataset\n",
    "import explicit_model\n",
    "\n",
    "from pydpf.distributions.Gaussian import MultivariateGaussian, LinearGaussian\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import training\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "data_folder =  Path('./data/test/')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Utility functions",
   "id": "fbeda75550869f9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T13:08:37.563871Z",
     "start_time": "2025-01-30T13:08:37.557859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#set the seed\n",
    "gen_num = 9\n",
    "\n",
    "#Easy way to create new generator objects to manage RNG\n",
    "def new_gen():\n",
    "    global gen_num\n",
    "    global device\n",
    "    gen_num += 1\n",
    "    return torch.Generator(device=device).manual_seed(gen_num)\n",
    "\n",
    "#Functions to create random matrices for parameter initialisation\n",
    "def get_spectral_radius(M):\n",
    "    eigvals = torch.linalg.eigvals(M)\n",
    "    return torch.max(torch.abs(eigvals))\n",
    "\n",
    "def make_random(size, range, sparsity, device, generator):\n",
    "    return torch.where(torch.rand(size, device=device, generator=generator) < sparsity, torch.rand(size, device=device, generator=generator) * (range[1] - range[0]) + range[0], 0) \n",
    "\n",
    "def make_random_matrix(*, data:Tensor=None, device:torch.device=None, force_diagonal:bool=False, size:Union[Tuple[int,int], int]=None, range:Tuple[float,float]=(0.,1.), diag_range:Tuple[float,float]=(0.,1.), off_diag_range:Tuple[float,float]=(0., 1.), max_radius:float=None, generator:torch.Generator=None, positive_definite:bool=False, requires_grad:bool = True, sparsity:float=1.):\n",
    "    if generator is None:\n",
    "        generator = torch.Generator(device=device)\n",
    "    if not data is None:\n",
    "        return torch.nn.Parameter(data, requires_grad)\n",
    "    if isinstance(size, int):\n",
    "        vec = make_random(size, range, sparsity, device, generator)\n",
    "        return torch.nn.Parameter(vec, requires_grad)\n",
    "    if size[0]==size[1]:\n",
    "        if positive_definite and diag_range[0]<0:\n",
    "            raise ValueError(\"Diagonal range must be positive for positive definite matrices\")\n",
    "        \n",
    "        diag = make_random(size[0], diag_range, sparsity, device, generator)\n",
    "        matrix = torch.diag(diag)\n",
    "        if not force_diagonal:\n",
    "            off_diag = make_random(size, off_diag_range, sparsity, device, generator) * (1-torch.eye(size[0], device=device))\n",
    "            matrix += off_diag\n",
    "            if positive_definite:\n",
    "                matrix = matrix.T @ matrix\n",
    "        if max_radius is not None:\n",
    "            radius = get_spectral_radius(matrix)\n",
    "            if radius > max_radius:\n",
    "                raise ValueError(f'Spectral radius {radius} exceeds maximum {max_radius}, consider decreasing the range or trying a different seed')\n",
    "        return torch.nn.Parameter(matrix, requires_grad)\n",
    "            \n",
    "    matrix = make_random(size, range, sparsity, device, generator)\n",
    "    return torch.nn.Parameter(matrix, requires_grad)\n",
    "\n",
    "def load_matrix_csv(path, name):\n",
    "    matrix_loc = path / name\n",
    "    return torch.tensor(np.loadtxt(matrix_loc, delimiter=','), device=device, dtype=torch.float32)\n",
    "    "
   ],
   "id": "d2e6bac920c53fd1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T13:08:39.760046Z",
     "start_time": "2025-01-30T13:08:39.286766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_loc = data_folder / 'data.csv'\n",
    "normalise = True\n",
    "\n",
    "#Load the data\n",
    "data_set = StateSpaceDataset(data_loc, state_prefix='state', device=device)\n",
    "initialisation_generator = new_gen()"
   ],
   "id": "62202735057694ee",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Parameter initialisation",
   "id": "8c4d52f0c1616599"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T13:08:42.656798Z",
     "start_time": "2025-01-30T13:08:42.531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Initialise model parameters as random Matrices\n",
    "prior_covariance = make_random_matrix(size = (data_set.state_dimension, data_set.state_dimension), diag_range=(0, 1), off_diag_range=(-1, 1), generator=initialisation_generator, device=device, positive_definite=True)\n",
    "dynamic_matrix = make_random_matrix(size=(data_set.state_dimension,data_set.state_dimension), diag_range=(-0.2, 0.2), off_diag_range=(-0.3, 0.3), generator=initialisation_generator, max_radius=1, device=device)\n",
    "dynamic_bias = make_random_matrix(size=data_set.state_dimension, range=(-0.3, 0.3), generator=initialisation_generator, device=device, requires_grad=False)\n",
    "dynamic_covariance = make_random_matrix(size = (data_set.state_dimension, data_set.state_dimension), diag_range=(0, 1), off_diag_range=(-1, 1), generator=initialisation_generator, device=device, positive_definite=True)\n",
    "if data_set.state_dimension==data_set.observation_dimension:\n",
    "    observation_matrix = make_random_matrix(size=(data_set.observation_dimension,data_set.state_dimension), diag_range=(-1, 1), off_diag_range=(-1, 1), generator=initialisation_generator, device=device)\n",
    "else:\n",
    "    observation_matrix = make_random_matrix(size=(data_set.observation_dimension,data_set.state_dimension), range=(-1, 1), generator=initialisation_generator, device=device)\n",
    "observation_bias = make_random_matrix(size=data_set.state_dimension, range=(-1, 1), generator=initialisation_generator, device=device)\n",
    "observation_covariance = make_random_matrix(size = (data_set.observation_dimension, data_set.observation_dimension), diag_range=(0, 1), off_diag_range=(-1, 1), generator=initialisation_generator, device=device, positive_definite=True)\n",
    "\n",
    "#Take the Cholesky decomposition of the covariances and set it in place to avoid adding to the computation graph\n",
    "prior_covariance.data = torch.linalg.cholesky(prior_covariance)\n",
    "dynamic_covariance.data = torch.linalg.cholesky(dynamic_covariance)\n",
    "observation_covariance.data = torch.linalg.cholesky(observation_covariance)\n",
    "\n",
    "\n"
   ],
   "id": "d1de389645530f16",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Defining the Model",
   "id": "cabaf9411959e4bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T13:09:10.696607Z",
     "start_time": "2025-01-30T13:09:10.690329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Define the model components as distribution objects\n",
    "prior_dist = MultivariateGaussian(mean = torch.zeros(data_set.state_dimension, device = device), cholesky_covariance=prior_covariance, generator=new_gen())\n",
    "dynamic_dist = LinearGaussian(weight = dynamic_matrix, bias = dynamic_bias, cholesky_covariance=dynamic_covariance, generator=new_gen(), constrain_spectral_radius=0.99)\n",
    "observation_dist = LinearGaussian(weight = observation_matrix, bias = observation_bias, cholesky_covariance=observation_covariance, generator=new_gen())\n",
    "\n",
    "#Register model componets as a FilteringModel\n",
    "SSM = pydpf.FilteringModel(dynamic_model=dynamic_dist, observation_model=observation_dist, prior_model=prior_dist)\n",
    "#Define a DPF to run the FilteringModel\n",
    "dpf = pydpf.DPF(SSM, new_gen())\n",
    "\n",
    "#kernel = pydpf.KernelMixture([('Gaussian', data_set.state_dimension)], generator=new_gen(), gradient_estimator='reparameterisation')\n",
    "#dpf = pydpf.KernelDPF(SSM, kernel)\n",
    "#dpf = pydpf.SVGDKernelDPF(SSM, kernel, iterations=10, lr=1, alpha=0.9)"
   ],
   "id": "be8d4bf7e4d20ebb",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Defining the model with generic API",
   "id": "41feccb9c664e8dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T13:09:26.215931Z",
     "start_time": "2025-01-30T13:09:26.210169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Define the model components as custom Modules\n",
    "prior_dist = explicit_model.GaussianPrior(mean = torch.zeros(data_set.state_dimension, device = device), cholesky_covariance=prior_covariance, generator=new_gen(), device=device)\n",
    "dynamic_dist = explicit_model.LinearGaussianDynamic(weight = dynamic_matrix, bias = dynamic_bias, cholesky_covariance=dynamic_covariance, device = device, generator=new_gen(), max_spectral_radius=0.99)\n",
    "observation_dist = explicit_model.LinearGaussianObservation(weight = observation_matrix, bias = observation_bias, cholesky_covariance=observation_covariance, device = device, generator=new_gen())\n",
    "\n",
    "#Register model componets as a FilteringModel\n",
    "SSM = pydpf.FilteringModel(dynamic_model=dynamic_dist, observation_model=observation_dist, prior_model=prior_dist)\n",
    "#Define a DPF to run the FilteringModel\n",
    "dpf = pydpf.DPF(SSM, new_gen())"
   ],
   "id": "cdc6fdb6bfe08db0",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "6c85cebf6c0f3198"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T13:09:34.792693Z",
     "start_time": "2025-01-30T13:09:27.570491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Train the model with a fairly standard torch training loop.\n",
    "#Use the Adam optimser; MSE loss; and clip the gradients elementwise each iteration.\n",
    "training.train(dpf, torch.optim.Adam(dpf.parameters(), lr=1e-2), data_set, 50, (50, 50, 1000), (30, -1, -1), (0.5, 0.25, 0.25), pydpf.MSE_Loss(), None, torch.Generator().manual_seed(0), pydpf.ClipByElement(1.))\n",
    "for n, p in dpf.named_parameters():\n",
    "    print(n)\n",
    "    print(p)"
   ],
   "id": "5dd254e7d3e9f503",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2799855172634125\n",
      "0.30497169494628906\n",
      "0.288920134305954\n",
      "0.29208987951278687\n",
      "0.28048259019851685\n",
      "0.28144848346710205\n",
      "0.2704857587814331\n",
      "0.29070618748664856\n",
      "0.27755555510520935\n",
      "0.27878737449645996\n",
      "0.28351888060569763\n",
      "0.2731468379497528\n",
      "0.2748449444770813\n",
      "0.27419334650039673\n",
      "0.26768821477890015\n",
      "0.26781439781188965\n",
      "0.2783457934856415\n",
      "epoch 1/50, train loss: 0.2803322196006775, validation loss: 0.26630860567092896\n",
      "0.27135053277015686\n",
      "0.26680663228034973\n",
      "0.26421424746513367\n",
      "0.26805683970451355\n",
      "0.2620486915111542\n",
      "0.26671698689460754\n",
      "0.26400309801101685\n",
      "0.2683543562889099\n",
      "0.2625987231731415\n",
      "0.2672463655471802\n",
      "0.2586526870727539\n",
      "0.25966522097587585\n",
      "0.2601434886455536\n",
      "0.2653212547302246\n",
      "0.26220208406448364\n",
      "0.25200918316841125\n",
      "0.24906449019908905\n",
      "epoch 2/50, train loss: 0.26312600314617157, validation loss: 0.251854807138443\n",
      "0.2623167335987091\n",
      "0.2568990886211395\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#Train the model with a fairly standard torch training loop.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#Use the Adam optimser; MSE loss; and clip the gradients elementwise each iteration.\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m training\u001B[38;5;241m.\u001B[39mtrain(dpf, torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(dpf\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-2\u001B[39m), data_set, \u001B[38;5;241m50\u001B[39m, (\u001B[38;5;241m50\u001B[39m, \u001B[38;5;241m50\u001B[39m, \u001B[38;5;241m1000\u001B[39m), (\u001B[38;5;241m30\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m), (\u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m0.25\u001B[39m, \u001B[38;5;241m0.25\u001B[39m), pydpf\u001B[38;5;241m.\u001B[39mMSE_Loss(), \u001B[38;5;28;01mNone\u001B[39;00m, torch\u001B[38;5;241m.\u001B[39mGenerator()\u001B[38;5;241m.\u001B[39mmanual_seed(\u001B[38;5;241m0\u001B[39m), pydpf\u001B[38;5;241m.\u001B[39mClipByElement(\u001B[38;5;241m1.\u001B[39m))\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m n, p \u001B[38;5;129;01min\u001B[39;00m dpf\u001B[38;5;241m.\u001B[39mnamed_parameters():\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28mprint\u001B[39m(n)\n",
      "File \u001B[1;32m~\\PycharmProjects\\dpf-baselining\\Linear Gaussian\\training.py:88\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(dpf, opt, data_set, epochs, n_particles, batch_size, split_size, loss_function, metric, data_loading_generator, gradient_regulariser)\u001B[0m\n\u001B[0;32m     86\u001B[0m dpf\u001B[38;5;241m.\u001B[39mupdate()\n\u001B[0;32m     87\u001B[0m opt\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 88\u001B[0m loss \u001B[38;5;241m=\u001B[39m dpf(n_particles[\u001B[38;5;241m0\u001B[39m], observation\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m, loss_function, observation\u001B[38;5;241m=\u001B[39mobservation, ground_truth\u001B[38;5;241m=\u001B[39mstate, gradient_regulariser \u001B[38;5;241m=\u001B[39m gradient_regulariser)\n\u001B[0;32m     89\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mmean()\n\u001B[0;32m     90\u001B[0m \u001B[38;5;28mprint\u001B[39m(loss\u001B[38;5;241m.\u001B[39mitem())\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\PycharmProjects\\dpf-baselining\\pydpf\\filtering.py:132\u001B[0m, in \u001B[0;36mSIS.forward\u001B[1;34m(self, n_particles, time_extent, aggregation_function, observation, gradient_regulariser, ground_truth, control, time, series_metadata)\u001B[0m\n\u001B[0;32m    130\u001B[0m prev_state \u001B[38;5;241m=\u001B[39m state\n\u001B[0;32m    131\u001B[0m prev_weight \u001B[38;5;241m=\u001B[39m weight\n\u001B[1;32m--> 132\u001B[0m state, weight, likelihood \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproposal(prev_state\u001B[38;5;241m=\u001B[39mstate, prev_weight\u001B[38;5;241m=\u001B[39mweight, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtime_data)\n\u001B[0;32m    133\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m gradient_regulariser \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    134\u001B[0m     state, weight \u001B[38;5;241m=\u001B[39m gradient_regulariser(state\u001B[38;5;241m=\u001B[39mstate, weight\u001B[38;5;241m=\u001B[39mweight, prev_state\u001B[38;5;241m=\u001B[39mprev_state, prev_weight\u001B[38;5;241m=\u001B[39mprev_weight)\n",
      "File \u001B[1;32m~\\PycharmProjects\\dpf-baselining\\pydpf\\filtering.py:199\u001B[0m, in \u001B[0;36mParticleFilter._register_functions.<locals>.pf_sampler\u001B[1;34m(prev_state, prev_weight, **data)\u001B[0m\n\u001B[0;32m    197\u001B[0m resampled_x, resampled_w \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresampler(prev_state, prev_weight)\n\u001B[0;32m    198\u001B[0m initial_likelihood \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mlogsumexp(resampled_w, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m--> 199\u001B[0m state, weight \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mSIRS_proposal(prev_state\u001B[38;5;241m=\u001B[39mresampled_x, prev_weight\u001B[38;5;241m=\u001B[39mresampled_w, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdata)\n\u001B[0;32m    200\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    201\u001B[0m     weight, likelihood \u001B[38;5;241m=\u001B[39m normalise(weight)\n",
      "File \u001B[1;32m~\\PycharmProjects\\dpf-baselining\\pydpf\\model_based_api.py:108\u001B[0m, in \u001B[0;36mFilteringModel.get_prop_IS.<locals>.prop\u001B[1;34m(prev_state, prev_weight, observation, **data)\u001B[0m\n\u001B[0;32m    107\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprop\u001B[39m(prev_state, prev_weight, observation, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdata):\n\u001B[1;32m--> 108\u001B[0m     new_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdynamic_model\u001B[38;5;241m.\u001B[39msample(prev_state \u001B[38;5;241m=\u001B[39m prev_state, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdata)\n\u001B[0;32m    109\u001B[0m     new_weight \u001B[38;5;241m=\u001B[39m prev_weight \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation_model\u001B[38;5;241m.\u001B[39mscore(new_state, observation \u001B[38;5;241m=\u001B[39m observation, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdata)\n\u001B[0;32m    110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m new_state, new_weight\n",
      "File \u001B[1;32m~\\PycharmProjects\\dpf-baselining\\Linear Gaussian\\explicit_model.py:43\u001B[0m, in \u001B[0;36mLinearGaussianDynamic.sample\u001B[1;34m(self, prev_state)\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msample\u001B[39m(\u001B[38;5;28mself\u001B[39m, prev_state:Tensor)\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39mTensor:\n\u001B[1;32m---> 43\u001B[0m     standard_sample \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn(prev_state\u001B[38;5;241m.\u001B[39msize(), device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice, generator\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerator)\n\u001B[0;32m     44\u001B[0m     \u001B[38;5;66;03m#No built-in way to do matrix vector products in pytorch :(\u001B[39;00m\n\u001B[0;32m     45\u001B[0m     mean \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconstrained_weight \u001B[38;5;241m@\u001B[39m prev_state\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m.\u001B[39msqueeze() \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the true data generating model",
   "id": "25c99a80096722f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T13:13:54.880275Z",
     "start_time": "2025-01-30T13:13:54.834695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#kernel = pydpf.KernelMixture([('Gaussian', data_set.state_dimension)], generator=new_gen(), gradient_estimator='reparameterisation')\n",
    "prior_covariance = load_matrix_csv(data_folder, 'prior_covariance.csv')\n",
    "dynamic_covariance = load_matrix_csv(data_folder, 'dynamic_covariance.csv')\n",
    "dynamic_bias = load_matrix_csv(data_folder, 'dynamic_bias.csv')\n",
    "dynamic_matrix = load_matrix_csv(data_folder, 'dynamic_matrix.csv')\n",
    "observation_bias = load_matrix_csv(data_folder, 'observation_bias.csv')\n",
    "observation_covariance = load_matrix_csv(data_folder, 'observation_covariance.csv')\n",
    "observation_matrix = load_matrix_csv(data_folder, 'observation_matrix.csv')\n",
    "prior_dist = MultivariateGaussian(mean = torch.zeros(data_set.state_dimension, device = device), cholesky_covariance=torch.linalg.cholesky(prior_covariance), generator=new_gen())\n",
    "dynamic_dist = LinearGaussian(weight = dynamic_matrix, bias = dynamic_bias, cholesky_covariance=torch.linalg.cholesky(dynamic_covariance), generator=new_gen(), constrain_spectral_radius=0.99)\n",
    "observation_dist = LinearGaussian(weight = observation_matrix, bias = observation_bias, cholesky_covariance=torch.linalg.cholesky(observation_covariance), generator=new_gen())\n",
    "SSM = pydpf.FilteringModel(dynamic_model=dynamic_dist, observation_model=observation_dist, prior_model=prior_dist)\n",
    "pf = pydpf.DPF(SSM, resampling_generator=new_gen())\n",
    "#pf = pydpf.SVGDKernelDPF(SSM, kernel, iterations=10, lr=1, alpha=0.9)"
   ],
   "id": "26a61177636dfaa5",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Run the true model",
   "id": "6ca57c72e4101dcf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T13:14:03.252057Z",
     "start_time": "2025-01-30T13:13:55.926749Z"
    }
   },
   "cell_type": "code",
   "source": "training.test(pf, data_set, 1000, 20, pydpf.MSE_Loss(), data_loading_generator=torch.Generator().manual_seed(10))",
   "id": "54508f4c10ecbf81",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \n",
      "loss = 0.19143067836761474\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

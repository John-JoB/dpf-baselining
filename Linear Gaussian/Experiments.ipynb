{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-24T14:11:38.548087Z",
     "start_time": "2025-03-24T14:11:37.064379Z"
    }
   },
   "source": [
    "import pydpf\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import Tuple, Union\n",
    "from pydpf.datautils import StateSpaceDataset\n",
    "import explicit_model\n",
    "\n",
    "from pydpf.distributions.Gaussian import MultivariateGaussian, LinearGaussian\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import training\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "data_folder =  Path('./data/test/')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Utility functions",
   "id": "fbeda75550869f9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T14:11:38.567737Z",
     "start_time": "2025-03-24T14:11:38.559812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#set the seed\n",
    "gen_num = 9\n",
    "\n",
    "#Easy way to create new generator objects to manage RNG\n",
    "def new_gen():\n",
    "    global gen_num\n",
    "    global device\n",
    "    gen_num += 1\n",
    "    return torch.Generator(device=device).manual_seed(gen_num)\n",
    "\n",
    "#Functions to create random matrices for parameter initialisation\n",
    "def get_spectral_radius(M):\n",
    "    eigvals = torch.linalg.eigvals(M)\n",
    "    return torch.max(torch.abs(eigvals))\n",
    "\n",
    "def make_random(size, range, sparsity, device, generator):\n",
    "    return torch.where(torch.rand(size, device=device, generator=generator) < sparsity, torch.rand(size, device=device, generator=generator) * (range[1] - range[0]) + range[0], 0) \n",
    "\n",
    "def make_random_matrix(*, data:Tensor=None, device:torch.device=None, force_diagonal:bool=False, size:Union[Tuple[int,int], int]=None, range:Tuple[float,float]=(0.,1.), diag_range:Tuple[float,float]=(0.,1.), off_diag_range:Tuple[float,float]=(0., 1.), max_radius:float=None, generator:torch.Generator=None, positive_definite:bool=False, requires_grad:bool = True, sparsity:float=1.):\n",
    "    if generator is None:\n",
    "        generator = torch.Generator(device=device)\n",
    "    if not data is None:\n",
    "        return torch.nn.Parameter(data, requires_grad)\n",
    "    if isinstance(size, int):\n",
    "        vec = make_random(size, range, sparsity, device, generator)\n",
    "        return torch.nn.Parameter(vec, requires_grad)\n",
    "    if size[0]==size[1]:\n",
    "        if positive_definite and diag_range[0]<0:\n",
    "            raise ValueError(\"Diagonal range must be positive for positive definite matrices\")\n",
    "        \n",
    "        diag = make_random(size[0], diag_range, sparsity, device, generator)\n",
    "        matrix = torch.diag(diag)\n",
    "        if not force_diagonal:\n",
    "            off_diag = make_random(size, off_diag_range, sparsity, device, generator) * (1-torch.eye(size[0], device=device))\n",
    "            matrix += off_diag\n",
    "            if positive_definite:\n",
    "                matrix = matrix.T @ matrix\n",
    "        if max_radius is not None:\n",
    "            radius = get_spectral_radius(matrix)\n",
    "            if radius > max_radius:\n",
    "                raise ValueError(f'Spectral radius {radius} exceeds maximum {max_radius}, consider decreasing the range or trying a different seed')\n",
    "        return torch.nn.Parameter(matrix, requires_grad)\n",
    "            \n",
    "    matrix = make_random(size, range, sparsity, device, generator)\n",
    "    return torch.nn.Parameter(matrix, requires_grad)\n",
    "\n",
    "def load_matrix_csv(path, name):\n",
    "    matrix_loc = path / name\n",
    "    return torch.tensor(np.loadtxt(matrix_loc, delimiter=','), device=device, dtype=torch.float32)\n",
    "    "
   ],
   "id": "d2e6bac920c53fd1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T14:11:39.103055Z",
     "start_time": "2025-03-24T14:11:38.609983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_loc = data_folder / 'data.csv'\n",
    "normalise = True\n",
    "\n",
    "#Load the data\n",
    "data_set = StateSpaceDataset(data_loc, state_prefix='state', device=device)\n",
    "initialisation_generator = new_gen()"
   ],
   "id": "62202735057694ee",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Parameter initialisation",
   "id": "8c4d52f0c1616599"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T14:11:39.235942Z",
     "start_time": "2025-03-24T14:11:39.109043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Initialise model parameters as random Matrices\n",
    "prior_covariance = make_random_matrix(size = (data_set.state_dimension, data_set.state_dimension), diag_range=(0, 1), off_diag_range=(-1, 1), generator=initialisation_generator, device=device, positive_definite=True)\n",
    "dynamic_matrix = make_random_matrix(size=(data_set.state_dimension,data_set.state_dimension), diag_range=(-0.2, 0.2), off_diag_range=(-0.3, 0.3), generator=initialisation_generator, max_radius=1, device=device)\n",
    "dynamic_bias = make_random_matrix(size=data_set.state_dimension, range=(-0.3, 0.3), generator=initialisation_generator, device=device, requires_grad=False)\n",
    "dynamic_covariance = make_random_matrix(size = (data_set.state_dimension, data_set.state_dimension), diag_range=(0, 1), off_diag_range=(-1, 1), generator=initialisation_generator, device=device, positive_definite=True)\n",
    "if data_set.state_dimension==data_set.observation_dimension:\n",
    "    observation_matrix = make_random_matrix(size=(data_set.observation_dimension,data_set.state_dimension), diag_range=(-1, 1), off_diag_range=(-1, 1), generator=initialisation_generator, device=device)\n",
    "else:\n",
    "    observation_matrix = make_random_matrix(size=(data_set.observation_dimension,data_set.state_dimension), range=(-1, 1), generator=initialisation_generator, device=device)\n",
    "observation_bias = make_random_matrix(size=data_set.state_dimension, range=(-1, 1), generator=initialisation_generator, device=device)\n",
    "observation_covariance = make_random_matrix(size = (data_set.observation_dimension, data_set.observation_dimension), diag_range=(0, 1), off_diag_range=(-1, 1), generator=initialisation_generator, device=device, positive_definite=True)\n",
    "\n",
    "#Take the Cholesky decomposition of the covariances and set it in place to avoid adding to the computation graph\n",
    "prior_covariance.data = torch.linalg.cholesky(prior_covariance)\n",
    "dynamic_covariance.data = torch.linalg.cholesky(dynamic_covariance)\n",
    "observation_covariance.data = torch.linalg.cholesky(observation_covariance)\n",
    "\n",
    "\n"
   ],
   "id": "d1de389645530f16",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Defining the Model",
   "id": "cabaf9411959e4bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T14:11:39.275236Z",
     "start_time": "2025-03-24T14:11:39.270701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Define the model components as distribution objects\n",
    "prior_dist = MultivariateGaussian(mean = torch.zeros(data_set.state_dimension, device = device), cholesky_covariance=prior_covariance, generator=new_gen())\n",
    "dynamic_dist = LinearGaussian(weight = dynamic_matrix, bias = dynamic_bias, cholesky_covariance=dynamic_covariance, generator=new_gen(), constrain_spectral_radius=0.99)\n",
    "observation_dist = LinearGaussian(weight = observation_matrix, bias = observation_bias, cholesky_covariance=observation_covariance, generator=new_gen())\n",
    "\n",
    "#Register model componets as a FilteringModel\n",
    "SSM = pydpf.FilteringModel(dynamic_model=dynamic_dist, observation_model=observation_dist, prior_model=prior_dist)\n",
    "#Define a DPF to run the FilteringModel\n",
    "dpf = pydpf.DPF(SSM, new_gen())\n",
    "\n",
    "#kernel = pydpf.KernelMixture([('Gaussian', data_set.state_dimension)], generator=new_gen(), gradient_estimator='reparameterisation')\n",
    "#dpf = pydpf.KernelDPF(SSM, kernel)\n",
    "#dpf = pydpf.SVGDKernelDPF(SSM, kernel, iterations=10, lr=1, alpha=0.9)"
   ],
   "id": "be8d4bf7e4d20ebb",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Defining the model with generic API",
   "id": "41feccb9c664e8dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T14:11:39.290282Z",
     "start_time": "2025-03-24T14:11:39.286051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Define the model components as custom Modules\n",
    "prior_dist = explicit_model.GaussianPrior(mean = torch.zeros(data_set.state_dimension, device = device), cholesky_covariance=prior_covariance, generator=new_gen(), device=device)\n",
    "dynamic_dist = explicit_model.LinearGaussianDynamic(weight = dynamic_matrix, bias = dynamic_bias, cholesky_covariance=dynamic_covariance, device = device, generator=new_gen(), max_spectral_radius=0.99)\n",
    "observation_dist = explicit_model.LinearGaussianObservation(weight = observation_matrix, bias = observation_bias, cholesky_covariance=observation_covariance, device = device, generator=new_gen())\n",
    "\n",
    "#Register model componets as a FilteringModel\n",
    "SSM = pydpf.FilteringModel(dynamic_model=dynamic_dist, observation_model=observation_dist, prior_model=prior_dist)\n",
    "#Define a DPF to run the FilteringModel\n",
    "dpf = pydpf.DPF(SSM, new_gen())"
   ],
   "id": "cdc6fdb6bfe08db0",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "6c85cebf6c0f3198"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T14:11:47.678133Z",
     "start_time": "2025-03-24T14:11:47.623449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Train the model with a fairly standard torch training loop.\n",
    "#Use the Adam optimser; MSE loss; and clip the gradients elementwise each iteration.\n",
    "training.train(dpf, torch.optim.Adam(dpf.parameters(), lr=1e-2), data_set, 50, (50, 50, 1000), (30, -1, -1), (0.5, 0.25, 0.25), pydpf.MSE_Loss(), None, torch.Generator().manual_seed(0), pydpf.ClipByElement(1.))\n",
    "for n, p in dpf.named_parameters():\n",
    "    print(n)\n",
    "    print(p)"
   ],
   "id": "5dd254e7d3e9f503",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GaussianPrior.sample() got an unexpected keyword argument 't'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#Train the model with a fairly standard torch training loop.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#Use the Adam optimser; MSE loss; and clip the gradients elementwise each iteration.\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m training\u001B[38;5;241m.\u001B[39mtrain(dpf, torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(dpf\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-2\u001B[39m), data_set, \u001B[38;5;241m50\u001B[39m, (\u001B[38;5;241m50\u001B[39m, \u001B[38;5;241m50\u001B[39m, \u001B[38;5;241m1000\u001B[39m), (\u001B[38;5;241m30\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m), (\u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m0.25\u001B[39m, \u001B[38;5;241m0.25\u001B[39m), pydpf\u001B[38;5;241m.\u001B[39mMSE_Loss(), \u001B[38;5;28;01mNone\u001B[39;00m, torch\u001B[38;5;241m.\u001B[39mGenerator()\u001B[38;5;241m.\u001B[39mmanual_seed(\u001B[38;5;241m0\u001B[39m), pydpf\u001B[38;5;241m.\u001B[39mClipByElement(\u001B[38;5;241m1.\u001B[39m))\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m n, p \u001B[38;5;129;01min\u001B[39;00m dpf\u001B[38;5;241m.\u001B[39mnamed_parameters():\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28mprint\u001B[39m(n)\n",
      "File \u001B[1;32m~\\PycharmProjects\\dpf-baselining\\Linear Gaussian\\training.py:88\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(dpf, opt, data_set, epochs, n_particles, batch_size, split_size, loss_function, metric, data_loading_generator, gradient_regulariser)\u001B[0m\n\u001B[0;32m     86\u001B[0m dpf\u001B[38;5;241m.\u001B[39mupdate()\n\u001B[0;32m     87\u001B[0m opt\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 88\u001B[0m loss \u001B[38;5;241m=\u001B[39m dpf(n_particles[\u001B[38;5;241m0\u001B[39m], observation\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m, loss_function, observation\u001B[38;5;241m=\u001B[39mobservation, ground_truth\u001B[38;5;241m=\u001B[39mstate, gradient_regulariser \u001B[38;5;241m=\u001B[39m gradient_regulariser)\n\u001B[0;32m     89\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mmean()\n\u001B[0;32m     90\u001B[0m \u001B[38;5;66;03m#print(loss.item())\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\PycharmProjects\\dpf-baselining\\pydpf\\filtering.py:131\u001B[0m, in \u001B[0;36mSIS.forward\u001B[1;34m(self, n_particles, time_extent, aggregation_function, observation, gradient_regulariser, ground_truth, control, time, series_metadata)\u001B[0m\n\u001B[0;32m    129\u001B[0m     gt_exists \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    130\u001B[0m time_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_time_data(\u001B[38;5;241m0\u001B[39m, observation \u001B[38;5;241m=\u001B[39m observation, control \u001B[38;5;241m=\u001B[39m control, time \u001B[38;5;241m=\u001B[39m time, series_metadata \u001B[38;5;241m=\u001B[39m series_metadata)\n\u001B[1;32m--> 131\u001B[0m state, weight, likelihood \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minitial_proposal(n_particles \u001B[38;5;241m=\u001B[39m n_particles, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtime_data)\n\u001B[0;32m    133\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_dict:\n\u001B[0;32m    134\u001B[0m     output \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[1;32m~\\PycharmProjects\\dpf-baselining\\pydpf\\filtering.py:273\u001B[0m, in \u001B[0;36mParticleFilter._register_functions.<locals>.initial_sampler\u001B[1;34m(n_particles, **data)\u001B[0m\n\u001B[0;32m    272\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minitial_sampler\u001B[39m(n_particles: \u001B[38;5;28mint\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdata):\n\u001B[1;32m--> 273\u001B[0m     state, weight \u001B[38;5;241m=\u001B[39m prior(n_particles\u001B[38;5;241m=\u001B[39mn_particles, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdata)\n\u001B[0;32m    274\u001B[0m     weight, likelihood \u001B[38;5;241m=\u001B[39m normalise(weight)\n\u001B[0;32m    275\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m state, weight, likelihood\n",
      "File \u001B[1;32m~\\PycharmProjects\\dpf-baselining\\pydpf\\filtering.py:224\u001B[0m, in \u001B[0;36mParticleFilter._register_functions.<locals>.prior\u001B[1;34m(n_particles, observation, **data)\u001B[0m\n\u001B[0;32m    223\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprior\u001B[39m(n_particles, observation, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdata):\n\u001B[1;32m--> 224\u001B[0m     state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mSSM\u001B[38;5;241m.\u001B[39mprior_model\u001B[38;5;241m.\u001B[39msample(batch_size \u001B[38;5;241m=\u001B[39m observation\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), n_particles \u001B[38;5;241m=\u001B[39m n_particles, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdata)\n\u001B[0;32m    225\u001B[0m     weight \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mSSM\u001B[38;5;241m.\u001B[39mobservation_model\u001B[38;5;241m.\u001B[39mscore(state \u001B[38;5;241m=\u001B[39m state, observation \u001B[38;5;241m=\u001B[39m observation, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdata)\n\u001B[0;32m    226\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m state, weight\n",
      "\u001B[1;31mTypeError\u001B[0m: GaussianPrior.sample() got an unexpected keyword argument 't'"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the true data generating model",
   "id": "25c99a80096722f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T14:11:40.511672700Z",
     "start_time": "2025-01-30T13:13:54.834695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#kernel = pydpf.KernelMixture([('Gaussian', data_set.state_dimension)], generator=new_gen(), gradient_estimator='reparameterisation')\n",
    "prior_covariance = load_matrix_csv(data_folder, 'prior_covariance.csv')\n",
    "dynamic_covariance = load_matrix_csv(data_folder, 'dynamic_covariance.csv')\n",
    "dynamic_bias = load_matrix_csv(data_folder, 'dynamic_bias.csv')\n",
    "dynamic_matrix = load_matrix_csv(data_folder, 'dynamic_matrix.csv')\n",
    "observation_bias = load_matrix_csv(data_folder, 'observation_bias.csv')\n",
    "observation_covariance = load_matrix_csv(data_folder, 'observation_covariance.csv')\n",
    "observation_matrix = load_matrix_csv(data_folder, 'observation_matrix.csv')\n",
    "prior_dist = MultivariateGaussian(mean = torch.zeros(data_set.state_dimension, device = device), cholesky_covariance=torch.linalg.cholesky(prior_covariance), generator=new_gen())\n",
    "dynamic_dist = LinearGaussian(weight = dynamic_matrix, bias = dynamic_bias, cholesky_covariance=torch.linalg.cholesky(dynamic_covariance), generator=new_gen(), constrain_spectral_radius=0.99)\n",
    "observation_dist = LinearGaussian(weight = observation_matrix, bias = observation_bias, cholesky_covariance=torch.linalg.cholesky(observation_covariance), generator=new_gen())\n",
    "SSM = pydpf.FilteringModel(dynamic_model=dynamic_dist, observation_model=observation_dist, prior_model=prior_dist)\n",
    "pf = pydpf.DPF(SSM, resampling_generator=new_gen())\n",
    "#pf = pydpf.SVGDKernelDPF(SSM, kernel, iterations=10, lr=1, alpha=0.9)"
   ],
   "id": "26a61177636dfaa5",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Run the true model",
   "id": "6ca57c72e4101dcf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T14:11:40.511672700Z",
     "start_time": "2025-01-30T13:13:55.926749Z"
    }
   },
   "cell_type": "code",
   "source": "training.test(pf, data_set, 1000, 20, pydpf.MSE_Loss(), data_loading_generator=torch.Generator().manual_seed(10))",
   "id": "54508f4c10ecbf81",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \n",
      "loss = 0.19143067836761474\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Pydpf Basics Tutorial\n",
    "This tutorial gives a basic overview of the differentiable particle filtering package, pydpf. The package is intended to implement the majority of differentiable filters currently available in the literature and provide a convenient API for training them. It is part of a larger project to develop a benchmarking suite for DPFs. This tutorial assumes good knowledge of pytorch.\n",
    "\n",
    "## Design principles\n",
    "1. Extensibility\n",
    "    It should be as easy as possible for a user to define and train a filter to their needs without the package getting in the way. It should generally be possible to extend the functionality we provide by passing values to package functions, rather than subclassing package classes.\n",
    "2. Pytorch semantics\n",
    "    This module integrates with pytorch. Typical torch semantics are preserved as much as possible.\n",
    "3. Learning functions not models.\n",
    "    Conceptually it is best to think of modules written for pydpf as learning an algorithm to achieve the given inference goal, not a specific state space model. This gives the user flexibility in structuring their model as best suited to their problem rather than having to conform to the package's syntax. Most particle filtering algorithms are implementable as 'models' that can be passed to our base SMC algorithms, rather than needing to design them from scratch. Conversely, this puts the responsibility of designing well-structured and correct filtering algorithms on the user.\n",
    "\n",
    "## Overarching design patterns\n",
    "\n",
    "### The pydpf.Module class\n",
    "pydpf has a custom Module class that extends torch.nn.Module. pydpf.Module subclasses can optionally contain an update() method that should be used to calculate derived quantities from, or constrain parameters after they have changed. We recommend that all custom modules defined for use with pydpf subclass pydpf.Module. update() is called recursively on submodules, for this reason it is safe for pydf.Module classes to have torch.nn.Module submodules but not vice versa. So, there is no issue in using built in torch modules. Consequently, it is not safe to directly set attributes of a Module, one should provide a safe set_attribute method for Module attributes. The exception to this rule is when using torch's (somewhat arcane) optimizers which update the parameters inplace. One should then make sure to call .update() on all root Modules following an Optimizer step.\n",
    "\n",
    "### Dataformat\n",
    "@Ben this is subject to change if required. The data-format currently required by the pydpf data loading faculties is a folder of .csv files where each file contains a trajectory. Each file should have two columns, labelled state and observations containing vectors in the format '[state_1, state_2, ..., state_n]' (quotes included). Other columns are permitted but will be ignored.  \n",
    "\n",
    "### Order of dimensions\n",
    "The order of dimensions shall be: time, batch, particle, state/observation-dimension. Frequently one or more of these dimensions shall not be present but they should not change relative order."
   ],
   "id": "1374b35513731711"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T11:34:17.765109Z",
     "start_time": "2024-10-11T11:34:13.809413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#imports\n",
    "import pydpf\n",
    "import torch\n",
    "from typing import Tuple\n",
    "from pydpf import Module\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "#set device, defaults to cuda if it is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#set data path\n",
    "data_path = \"./data\""
   ],
   "id": "7a27496c771ae113",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Defining the true Model\n",
    "This tutorial takes the user through a simple two-dimensional Linear Gaussian example.\n",
    "At time = 0 the state is drawn from a Gaussian, at subsequent time-steps it is drawn from a Gaussian Markov kernel with linear dependence on the previous state. The observations are drawn from a different linear Gaussian forward kernel. The linear transforms and covariance matrices are constant for all time steps. "
   ],
   "id": "801d3cc4d4ba42b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T11:34:17.995916Z",
     "start_time": "2024-10-11T11:34:17.780485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Define the true parameters\n",
    "true_prior_mean = torch.zeros(size = (2,), dtype = torch.float32, device = device)\n",
    "true_prior_cholesky_cov = torch.eye(2, dtype = torch.float32, device = device)\n",
    "true_dynamic_scaling = torch.eye(n=2, dtype = torch.float32, device = device) * 0.9\n",
    "true_dynamic_offset = torch.tensor([0.2, 0.1], dtype = torch.float32, device = device)\n",
    "true_dynamic_cholesky_cov = torch.tensor([[0.1, 0], [0.03, 0.1]], dtype = torch.float32, device = device)\n",
    "true_observation_scaling = torch.eye(n = 2, dtype = torch.float32, device = device)\n",
    "true_observation_offset = torch.tensor([0, 0], dtype = torch.float32, device = device)\n",
    "true_observation_cholesky_cov = torch.eye(1, dtype = torch.float32, device = device)*0.05"
   ],
   "id": "d820bb644105c242",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## The distributions module\n",
    "We include a distributions module to implement common distributions (and conditional distributions) with a convenient API. Like the torch.distributions package it is not intended for the user to subclass the Distribution object, if they wish to implement their own it will always be easier to manually define sampling and density evaluation methods. Distribution objects implement a sample() and log_density() method, the specific arguments of which depend on the specific distribution. Distributions are pydpf.Module subclasses so may be assigned parameters. Distributions give three built in options for the gradient estimator to use for sampling, 'reparameterisation' reparameterisation trick, 'score' score-based/REINFORCE, and 'none' detach gradients. Although, attaching a score-based gradient estimator to the sample requires doing so in linear-space, whereas attaching it to the importance weights can be done in log-space so may be more stable in practice."
   ],
   "id": "205f7b890fe98f3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T11:34:18.466851Z",
     "start_time": "2024-10-11T11:34:18.223834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "true_prior = pydpf.distributions.MultivariateGaussian(device = device, gradient_estimator = 'none', generator = None, mean = true_prior_mean, cholesky_covariance = true_prior_cholesky_cov)\n",
    "\n",
    "true_dynamic = pydpf.distributions.LinearGaussian(device = device, gradient_estimator='none', generator= None, weight =  true_dynamic_scaling, bias = true_dynamic_offset, cholesky_covariance = true_dynamic_cholesky_cov)\n",
    "\n",
    "true_observation = pydpf.distributions.LinearGaussian(device = device, gradient_estimator='none', generator= None, weight =  true_observation_scaling, bias = true_observation_offset, cholesky_covariance = true_observation_cholesky_cov)"
   ],
   "id": "8daa135a74e0fe05",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Saving simulated data\n",
    "We provide a method to generate a set of trajectories and save them to folder as .csv files as required by our format. The simulate_to_folder method takes methods to sample from the prior and the dynamic and observation kernels.\n",
    "\n",
    "In this case the sampling functions are the .sample() methods of the distributions defined above.\n",
    "\n",
    "Note: sometimes running joblib processes in a jupyter notebook crashes, just rerun the cell should this happen."
   ],
   "id": "2cee0e9c68d8a48d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T11:34:38.860029Z",
     "start_time": "2024-10-11T11:34:29.378664Z"
    }
   },
   "cell_type": "code",
   "source": "pydpf.simulate_to_folder(data_path, true_prior.sample, true_dynamic.sample, true_observation.sample, time_extent=50, n_trajectories=1000, batch_size=30, device=device, processes=-1)",
   "id": "3b8857bbfe177867",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - folder already exists at ./data, continuing could overwrite its data\n",
      "Saving batch     1/34\r"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31m_RemoteTraceback\u001B[0m                          Traceback (most recent call last)",
      "\u001B[1;31m_RemoteTraceback\u001B[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\jjbra\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 426, in _process_worker\n    call_item = call_queue.get(block=True, timeout=timeout)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\jjbra\\anaconda3\\Lib\\multiprocessing\\queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\jjbra\\anaconda3\\Lib\\site-packages\\torch\\storage.py\", line 381, in _load_from_bytes\n    return torch.load(io.BytesIO(b))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\jjbra\\anaconda3\\Lib\\site-packages\\torch\\serialization.py\", line 1040, in load\n    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\jjbra\\anaconda3\\Lib\\site-packages\\torch\\serialization.py\", line 1272, in _legacy_load\n    result = unpickler.load()\n             ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\jjbra\\anaconda3\\Lib\\site-packages\\torch\\serialization.py\", line 1205, in persistent_load\n    obj = restore_location(obj, location)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\jjbra\\anaconda3\\Lib\\site-packages\\torch\\serialization.py\", line 390, in default_restore_location\n    result = fn(storage, location)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\jjbra\\anaconda3\\Lib\\site-packages\\torch\\serialization.py\", line 268, in _cuda_deserialize\n    return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mBrokenProcessPool\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m pydpf\u001B[38;5;241m.\u001B[39msimulate_to_folder(data_path, true_prior\u001B[38;5;241m.\u001B[39msample, true_dynamic\u001B[38;5;241m.\u001B[39msample, true_observation\u001B[38;5;241m.\u001B[39msample, time_extent\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m, n_trajectories\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m30\u001B[39m, device\u001B[38;5;241m=\u001B[39mdevice, processes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\dpf-baselining\\pydpf\\datautils.py:115\u001B[0m, in \u001B[0;36msimulate_to_folder\u001B[1;34m(dir_path, prior, Markov_kernel, observation_model, time_extent, n_trajectories, batch_size, device, processes)\u001B[0m\n\u001B[0;32m    113\u001B[0m             observation[t\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m observation_model(state[t\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m], t\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    114\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSaving batch     \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbatch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mn_batches\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\r\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 115\u001B[0m         Parallel(n_jobs\u001B[38;5;241m=\u001B[39mprocesses)(delayed(\u001B[38;5;28;01mlambda\u001B[39;00m traj_index_: write_helper(traj_index_, batch_size \u001B[38;5;241m*\u001B[39m batch, dir_path, state, observation))(traj_index) \u001B[38;5;28;01mfor\u001B[39;00m traj_index \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(observation\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m1\u001B[39m)))\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDone                  \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   2001\u001B[0m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[0;32m   2002\u001B[0m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[0;32m   2003\u001B[0m \u001B[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001B[39;00m\n\u001B[0;32m   2004\u001B[0m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[0;32m   2005\u001B[0m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 2007\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(output)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001B[0m, in \u001B[0;36mParallel._get_outputs\u001B[1;34m(self, iterator, pre_dispatch)\u001B[0m\n\u001B[0;32m   1647\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[0;32m   1649\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[1;32m-> 1650\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrieve()\n\u001B[0;32m   1652\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[0;32m   1653\u001B[0m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[0;32m   1654\u001B[0m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[0;32m   1655\u001B[0m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[0;32m   1656\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1754\u001B[0m, in \u001B[0;36mParallel._retrieve\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait_retrieval():\n\u001B[0;32m   1748\u001B[0m \n\u001B[0;32m   1749\u001B[0m     \u001B[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m     \u001B[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001B[39;00m\n\u001B[0;32m   1751\u001B[0m     \u001B[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001B[39;00m\n\u001B[0;32m   1752\u001B[0m     \u001B[38;5;66;03m# worker traceback.\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_aborting:\n\u001B[1;32m-> 1754\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raise_error_fast()\n\u001B[0;32m   1755\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m   1757\u001B[0m     \u001B[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m     \u001B[38;5;66;03m# async callbacks to progress.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1789\u001B[0m, in \u001B[0;36mParallel._raise_error_fast\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1785\u001B[0m \u001B[38;5;66;03m# If this error job exists, immediately raise the error by\u001B[39;00m\n\u001B[0;32m   1786\u001B[0m \u001B[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001B[39;00m\n\u001B[0;32m   1787\u001B[0m \u001B[38;5;66;03m# called directly or if the generator is gc'ed.\u001B[39;00m\n\u001B[0;32m   1788\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m error_job \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1789\u001B[0m     error_job\u001B[38;5;241m.\u001B[39mget_result(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:745\u001B[0m, in \u001B[0;36mBatchCompletionCallBack.get_result\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    739\u001B[0m backend \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparallel\u001B[38;5;241m.\u001B[39m_backend\n\u001B[0;32m    741\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m backend\u001B[38;5;241m.\u001B[39msupports_retrieve_callback:\n\u001B[0;32m    742\u001B[0m     \u001B[38;5;66;03m# We assume that the result has already been retrieved by the\u001B[39;00m\n\u001B[0;32m    743\u001B[0m     \u001B[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001B[39;00m\n\u001B[0;32m    744\u001B[0m     \u001B[38;5;66;03m# be returned.\u001B[39;00m\n\u001B[1;32m--> 745\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_return_or_raise()\n\u001B[0;32m    747\u001B[0m \u001B[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001B[39;00m\n\u001B[0;32m    748\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:763\u001B[0m, in \u001B[0;36mBatchCompletionCallBack._return_or_raise\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    761\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    762\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m==\u001B[39m TASK_ERROR:\n\u001B[1;32m--> 763\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\n\u001B[0;32m    764\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\n\u001B[0;32m    765\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "\u001B[1;31mBrokenProcessPool\u001B[0m: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable."
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Initialising model parameters\n",
    "In this example we will only learn the dynamic and observation models, not the prior. We randomly initialise the parameters as torch.nn.Parameters and define new distributions to hold them."
   ],
   "id": "50b0b92a2e0de33c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T11:34:40.816712Z",
     "start_time": "2024-10-11T11:34:40.793201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "initialisation_generator = torch.Generator(device = device).manual_seed(0)\n",
    "learned_dynamic_scaling = torch.nn.Parameter(torch.rand(size = (2,2), dtype = torch.float32, device = device, generator=initialisation_generator) * 2 - 1, requires_grad = True)\n",
    "learned_dynamic_offset = torch.nn.Parameter(torch.rand(size = (2,), dtype = torch.float32, device = device, generator=initialisation_generator) * 2 - 1, requires_grad = True, )\n",
    "learned_dynamic_cholesky_cov = torch.nn.Parameter(torch.rand(size = (2,2), dtype = torch.float32, device = device, generator=initialisation_generator), requires_grad = True)\n",
    "learned_observation_scaling = torch.nn.Parameter(torch.rand(size = (2,2), dtype = torch.float32, device = device, generator=initialisation_generator) * 2 - 1, requires_grad = True)\n",
    "learned_observation_offset = torch.nn.Parameter(torch.rand(size = (2,), dtype = torch.float32, device = device, generator=initialisation_generator) * 2 - 1, requires_grad = True)\n",
    "learned_observation_cholesky_cov = torch.nn.Parameter(torch.rand(size = (2,2), dtype = torch.float32, device = device, generator=initialisation_generator), requires_grad = True)\n",
    "\n",
    "prior_rng = torch.Generator(device = device)\n",
    "prior_rng.manual_seed(0)\n",
    "dynamic_rng = torch.Generator(device = device)\n",
    "dynamic_rng.manual_seed(1)\n",
    "observation_rng = torch.Generator(device = device)\n",
    "observation_rng.manual_seed(2)\n",
    "\n",
    "\n",
    "filtering_prior = true_prior\n",
    "filtering_prior.set_generator(prior_rng)\n",
    "\n",
    "filtering_dynamic = pydpf.distributions.LinearGaussian(device = device, gradient_estimator = 'reparameterisation', generator = dynamic_rng, weight = learned_dynamic_scaling, bias = learned_dynamic_offset, cholesky_covariance = learned_dynamic_cholesky_cov)\n",
    "\n",
    "filtering_observation = pydpf.distributions.LinearGaussian(device = device, gradient_estimator = 'reparameterisation', generator = observation_rng, weight = learned_dynamic_scaling, bias = learned_dynamic_offset, cholesky_covariance = learned_dynamic_cholesky_cov)"
   ],
   "id": "fa371e34e586adf0",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Defining the DPF\n",
    "ParticleFilter is our implementation of a (not necessarily differentiable) particle filter. The particle filter is defined in terms of three functions. A function to importance sample posterior at time zero; takes the number of particles and the observation at time zero. A function to importance sample from the posterior at subsequent time-steps; takes the previous states, the previous weights, the observations at the current time and the current time. And a resampling algorithm; takes the particles and weights and returns the resampled particles, weights and another tensor (used to pass other information for outputting, most often the resampled indices).\n",
    "\n",
    "In our example we demonstrate the usage of the .update() function by using it to constrain the dynamic matrix's spectral index. For this example we employ the non-differentiable multinomial resampling."
   ],
   "id": "d257f27a75eefe1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T11:34:43.444120Z",
     "start_time": "2024-10-11T11:34:43.435840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PriorSampler(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prior_model = filtering_prior\n",
    "        self.observation_model = filtering_observation\n",
    "\n",
    "    def forward(self, n_particles: int, data: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        new_state = self.prior_model.sample((data.size(0), n_particles))\n",
    "        new_weights = self.observation_model.log_density(data.unsqueeze(1), new_state)\n",
    "        return new_state, new_weights\n",
    "    \n",
    "\n",
    "class FilteringSampler(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prior_model = filtering_prior\n",
    "        self.dynamic_model = filtering_dynamic\n",
    "        self.observation_model = filtering_observation\n",
    "        \n",
    "    def forward(self, state: torch.Tensor, weights: torch.Tensor, data: torch.Tensor, time: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        new_state = self.dynamic_model.sample(state)\n",
    "        new_weights = self.observation_model.log_density(data.unsqueeze(1), new_state) + weights\n",
    "        return new_state, new_weights\n",
    "        \n",
    "    def update(self):\n",
    "        super().update()\n",
    "        #Force the spectral radius of the dynamic matrix to be <= 1\n",
    "        #Prevents exponentially, but not linear diverging state\n",
    "        #Implictly assumes that the spectral index of the true dynamic matrix is <= 1\n",
    "        with torch.no_grad():\n",
    "            dynamic_eigvals = torch.linalg.eigvals(self.dynamic_model.weight)\n",
    "            dynamic_spectral_radius = torch.max(torch.abs(dynamic_eigvals))\n",
    "            if dynamic_spectral_radius > 1:\n",
    "                self.dynamic_model.set_weight(self.dynamic_model.weight/dynamic_spectral_radius)\n",
    "\n",
    "\n",
    "dpf = pydpf.ParticleFilter(PriorSampler(), pydpf.multinomial, FilteringSampler())"
   ],
   "id": "2895aacda6d65240",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Loading data\n",
    "Data loading closely follows pytorch syntax, we have defined a custom dataset to load data from a folder of formatted .csv files. The dataset can then be treated as a regular pytorch dataset. With the exception that one should pass its .collate() routine to the DataLoader collate_fn argument."
   ],
   "id": "26c56b69ffb46a8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T11:34:54.949178Z",
     "start_time": "2024-10-11T11:34:47.000683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_generator = torch.Generator(device='cpu').manual_seed(3)\n",
    "train_loader_generator = torch.Generator(device='cpu').manual_seed(4)\n",
    "validation_loader_generator = torch.Generator(device='cpu').manual_seed(5)\n",
    "dataset = pydpf.StateSpaceDataset(data_path, device=device)\n",
    "train_set, validation_set = torch.utils.data.random_split(dataset, [500, 500], generator=dataset_generator)\n",
    "train_dataloader = DataLoader(train_set, batch_size=32, shuffle=True, generator=train_loader_generator, collate_fn=dataset.collate)\n",
    "validation_dataloader = DataLoader(validation_set, batch_size=128, shuffle=True, generator=validation_loader_generator, collate_fn=dataset.collate)"
   ],
   "id": "6341c85c2afc7669",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T11:34:54.957124Z",
     "start_time": "2024-10-11T11:34:54.953011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Hyper-parameters\n",
    "epochs = 20\n",
    "lr = 0.001\n",
    "n_particles_train = 100\n",
    "n_particles_validation = 1000"
   ],
   "id": "6cd2f6841b209b9c",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training loop\n",
    "It is often crucial to have control over the training loop to debug and tune models so, like base pytorch, we leave their implementation to the user. The simple algorithm in this example is not very good, it is only for the point of demonstration, much better losses for this example are possible."
   ],
   "id": "783c3807f61fa898"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T11:35:34.473147Z",
     "start_time": "2024-10-11T11:34:54.978390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "opt = torch.optim.SGD(dpf.parameters(), lr=lr)\n",
    "for epoch in range(epochs):\n",
    "    train_loss = []\n",
    "    for state, observation in train_dataloader:\n",
    "        opt.zero_grad()\n",
    "        filtering_means = dpf(observation, n_particles_train, observation.size(0)-1, pydpf.filtering_mean())\n",
    "        loss = pydpf.MSE_loss(filtering_means, state)\n",
    "        loss.backward()\n",
    "        train_loss.append(loss.item())\n",
    "        opt.step()\n",
    "        dpf.update()\n",
    "    train_loss = np.mean(np.array(train_loss))\n",
    "        \n",
    "    with torch.inference_mode():\n",
    "        for state, observation in validation_dataloader:\n",
    "            filtering_means = dpf(observation, n_particles_validation, observation.size(0) - 1, pydpf.filtering_mean())\n",
    "            loss = pydpf.MSE_loss(filtering_means, state)\n",
    "            validation_loss = loss.item()\n",
    "    print('                                                                                                    ', end='\\r')\n",
    "    print(f'epoch {epoch+1}/{epochs}, train loss: {train_loss}, validation loss: {validation_loss}', end='\\r')\n",
    "print('')   "
   ],
   "id": "fb5954232952e84d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20/20, train loss: 1.9937222190201283, validation loss: 0.9313677549362183\r                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \r\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
